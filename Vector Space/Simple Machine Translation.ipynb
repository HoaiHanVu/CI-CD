{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd7285cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vuhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/vuhan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "from functions import (cosine_similarity, get_dict, process_tweet)\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73cbf44",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "\n",
    "# 1. The Word Embeddings Data for English and French Words\n",
    "\n",
    "Write a program that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80c0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b23682",
   "metadata": {},
   "source": [
    "#### Look at the data\n",
    "\n",
    "* en_embeddings_subset: the key is an English word, and the value is a\n",
    "300 dimensional array, which is the embedding for that word.\n",
    "```\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "```\n",
    "\n",
    "* fr_embeddings_subset: the key is a French word, and the value is a 300\n",
    "dimensional array, which is the embedding for that word.\n",
    "```\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7305d5",
   "metadata": {},
   "source": [
    "#### Load two dictionaries mapping the English to French words\n",
    "* A training dictionary\n",
    "* and a testing dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0de8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('./data/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd694a",
   "metadata": {},
   "source": [
    "#### Looking at the English French dictionary\n",
    "\n",
    "* `en_fr_train` is a dictionary where the key is the English word and the value\n",
    "is the French translation of that English word.\n",
    "```\n",
    "{'the': 'la',\n",
    " 'and': 'et',\n",
    " 'was': 'Ã©tait',\n",
    " 'for': 'pour',\n",
    "```\n",
    "\n",
    "* `en_fr_test` is similar to `en_fr_train`, but is a test set.  We won't look at it\n",
    "until we get to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafa29f",
   "metadata": {},
   "source": [
    "<a name=\"1-1\"></a>\n",
    "### 1.1 Generate Embedding and Transform Matrices\n",
    "\n",
    "<a name=\"ex-1\"></a>\n",
    "### Function `get_matrices`\n",
    "\n",
    "Translating English dictionary to French by using embeddings.\n",
    "\n",
    "Implement a function `get_matrices`, which takes the loaded data\n",
    "and returns matrices `X` and `Y`.\n",
    "\n",
    "Inputs:\n",
    "- `en_fr` : English to French dictionary\n",
    "- `en_embeddings` : English to embeddings dictionary\n",
    "- `fr_embeddings` : French to embeddings dictionary\n",
    "\n",
    "Returns:\n",
    "- Matrix `X` and matrix `Y`, where each row in X is the word embedding for an\n",
    "english word, and the same row in Y is the word embedding for the French\n",
    "version of that English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8c1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, fr_vecs, eng_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        fr_vecs: French words to their corresponding word embeddings.\n",
    "        eng_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst_X = list()\n",
    "    lst_Y = list()\n",
    "    \n",
    "    eng_set = set(eng_vecs.keys())\n",
    "    fren_set = set(fr_vecs.keys())\n",
    "    fren_words = set(en_fr.values())\n",
    "    \n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if en_word in eng_set and fr_word in fren_set:\n",
    "            en_vec = eng_vecs[en_word]\n",
    "            fr_vec = fr_vecs[fr_word]\n",
    "            \n",
    "            lst_X.append(en_vec)\n",
    "            lst_Y.append(fr_vec)\n",
    "            \n",
    "    X = np.vstack(lst_X)\n",
    "    Y = np.vstack(lst_Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45700a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae59e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4932, 300)\n",
      "(4932, 300)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2838b02",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "# 2. Translations\n",
    "\n",
    "Write a program that translates English words to French words using word embeddings and vector space models. \n",
    "\n",
    "<a name=\"2-1\"></a>\n",
    "### 2.1 Translation as Linear Transformation of Embeddings\n",
    "\n",
    "Given dictionaries of English and French word embeddings then create a transformation matrix `R`\n",
    "* Given an English word embedding, $\\mathbf{e}$, multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
    "    * Both $\\mathbf{e}$ and $\\mathbf{f}$ are [row vectors](https://en.wikipedia.org/wiki/Row_and_column_vectors).\n",
    "* We can then compute the nearest neighbors to `f` in the french embeddings and recommend the word that is most similar to the transformed word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f484134",
   "metadata": {},
   "source": [
    "#### Describing translation as the minimization problem\n",
    "\n",
    "Find a matrix `R` that minimizes the following equation. \n",
    "\n",
    "$$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $$\n",
    "\n",
    "#### Frobenius norm\n",
    "\n",
    "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
    "\n",
    "$$\\|\\mathbf{A}\\|_{F} \\equiv \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left|a_{i j}\\right|^{2}}\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5cbaa",
   "metadata": {},
   "source": [
    "#### Actual loss function\n",
    "In the real world applications, the Frobenius norm loss:\n",
    "\n",
    "$$\\| \\mathbf{XR} - \\mathbf{Y}\\|_{F}$$\n",
    "\n",
    "is often replaced by it's squared value divided by $m$:\n",
    "\n",
    "$$ \\frac{1}{m} \\|  \\mathbf{X R} - \\mathbf{Y} \\|_{F}^{2}$$\n",
    "\n",
    "where $m$ is the number of examples (rows in $\\mathbf{X}$).\n",
    "\n",
    "* The same R is found when using this loss function versus the original Frobenius norm.\n",
    "* The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.\n",
    "* The reason for dividing by $m$ is that we're more interested in the average loss per embedding than the  loss for the entire training set.\n",
    "    * The loss for all training set increases with more words (training examples),\n",
    "    so taking the average helps us to track the average loss regardless of the size of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478c94a",
   "metadata": {},
   "source": [
    "##### Detailed explanation why we use norm squared instead of the norm:\n",
    "\n",
    "- The norm is always nonnegative (we're summing up absolute values), and so is the square.\n",
    "- When we take the square of all non-negative (positive or zero) numbers, the order of the data is preserved.\n",
    "- For example, if 3 > 2, 3^2 > 2^2\n",
    "- Using the norm or squared norm in gradient descent results in the same location of the minimum.\n",
    "- Squaring cancels the square root in the Frobenius norm formula. Because of the chain rule, we would have to do more calculations if we had a square root in our expression for summation.\n",
    "- Dividing the function value by the positive number doesn't change the optimum of the function, for the same reason as described above.\n",
    "- We're interested in transforming English embedding into the French. Thus, it is more important to measure average loss per embedding than the loss for the entire dictionary (which increases as the number of words in the dictionary increases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6dd0a",
   "metadata": {},
   "source": [
    "#### Implementing translation mechanism described in this section.\n",
    "\n",
    "\n",
    "### Function `compute_loss`\n",
    "\n",
    "#### Step 1: Computing the loss\n",
    "* The loss function will be squared Frobenius norm of the difference between\n",
    "matrix and its approximation, divided by the number of training examples $m$.\n",
    "* Its formula is:\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa9a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    diff = np.dot(X, R) - Y\n",
    "    \n",
    "    diff_squared = np.square(diff)\n",
    "    \n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "    \n",
    "    loss = sum_diff_squared / m\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cdebd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for experiment with random matrix: 6.56\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "\n",
    "exp_loss = compute_loss(X, Y, R)\n",
    "\n",
    "print('Expected loss for experiment with random matrix: {:.2f}'.format(exp_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e0d1b",
   "metadata": {},
   "source": [
    "\n",
    "### Function `compute_gradient`\n",
    "\n",
    "#### Step 2: Computing the gradient of loss with respect to transform matrix R\n",
    "\n",
    "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
    "* The gradient is a matrix that encodes how much a small change in `R`\n",
    "affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease `R`\n",
    "to minimize the loss.\n",
    "* $m$ is the number of training examples (number of rows in $X$).\n",
    "* The formula for the gradient of the loss function $ð¿(ð,ð,ð)$ is:\n",
    "\n",
    "$$\\frac{d}{dR}ð¿(ð,ð,ð)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5040e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a scalar value - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    gradient = np.dot(X.T, (np.dot(X, R) - Y)) * 2 / m\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7bb20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of gradient matrix: [0.8509425  1.17397439 0.82708681 1.06200256 1.03393491]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = np.random.rand(n, n)\n",
    "\n",
    "exp_gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "print('First row of gradient matrix: {}'.format(exp_gradient[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afe6ce",
   "metadata": {},
   "source": [
    "### Function `align_embeddings`\n",
    "\n",
    "#### Step 3: Finding the optimal R with Gradient Descent Algorithm\n",
    "\n",
    "##### Gradient Descent\n",
    "\n",
    "[Gradient descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) is an iterative algorithm which is used in searching for the optimum of the function. \n",
    "* Earlier, we've mentioned that the gradient of the loss with respect to the matrix encodes how much a tiny change in some coordinate of that matrix affect the change of loss function.\n",
    "* Gradient descent uses that information to iteratively change matrix `R` until we reach a point where the loss is minimized. \n",
    "\n",
    "Pseudocode:\n",
    "1. Calculate gradient $g$ of the loss with respect to the matrix $R$.\n",
    "2. Update $R$ with the formula:\n",
    "$$R_{\\text{new}}= R_{\\text{old}}-\\alpha g$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate, which is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d56772",
   "metadata": {},
   "source": [
    "#### Training with a fixed number of iterations\n",
    "\n",
    "Most of the time we iterate for a fixed number of training steps rather than iterating until the loss falls below a threshold.\n",
    "\n",
    "##### Explanation for fixed number of iterations\n",
    "\n",
    "- You cannot rely on training loss getting low -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as \"early stopping\" -- until the validation accuracy starts to go down, which is a sign of over-fitting.\n",
    "- Why not always do \"early stopping\"? Well, mostly because well-regularized models on larger data-sets never stop improving. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?\n",
    "- Stopping after a certain number of steps has the advantage that you know how long your training will take - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. Another advantage is that you can fix your learning rate schedule -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4c0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, num_iterations=300, learning_rate=0.0005, verbose=True, \n",
    "                     compute_loss=compute_loss, compute_gradient=compute_gradient):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        num_iterations: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        if verbose and i % 25 == 0:\n",
    "            print('Loss at iteration {} is: {:.4f}'.format(i, compute_loss(X, Y, R)))\n",
    "        \n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        \n",
    "        R = R - (learning_rate * gradient)\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a85ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 is: 4.9403\n",
      "Loss at iteration 25 is: 4.6943\n",
      "Loss at iteration 50 is: 4.4608\n",
      "Loss at iteration 75 is: 4.2393\n",
      "Loss at iteration 100 is: 4.0290\n",
      "Loss at iteration 125 is: 3.8294\n",
      "Loss at iteration 150 is: 3.6400\n",
      "Loss at iteration 175 is: 3.4602\n",
      "Loss at iteration 200 is: 3.2895\n",
      "Loss at iteration 225 is: 3.1276\n",
      "Loss at iteration 250 is: 2.9738\n",
      "Loss at iteration 275 is: 2.8279\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "\n",
    "exp_R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336f403",
   "metadata": {},
   "source": [
    "#### Calculate Transformation matrix R\n",
    "\n",
    "Using just the training set, find the transformation matrix $\\mathbf{R}$ by calling the function `align_embeddings()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31971211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0 is: 966.0546\n",
      "Loss at iteration 25 is: 80.9353\n",
      "Loss at iteration 50 is: 20.2760\n",
      "Loss at iteration 75 is: 7.0565\n",
      "Loss at iteration 100 is: 3.1239\n",
      "Loss at iteration 125 is: 1.7019\n",
      "Loss at iteration 150 is: 1.1137\n",
      "Loss at iteration 175 is: 0.8461\n",
      "Loss at iteration 200 is: 0.7157\n",
      "Loss at iteration 225 is: 0.6488\n",
      "Loss at iteration 250 is: 0.6130\n",
      "Loss at iteration 275 is: 0.5932\n",
      "Loss at iteration 300 is: 0.5819\n",
      "Loss at iteration 325 is: 0.5753\n",
      "Loss at iteration 350 is: 0.5713\n",
      "Loss at iteration 375 is: 0.5689\n",
      "Loss at iteration 400 is: 0.5674\n",
      "Loss at iteration 425 is: 0.5664\n",
      "Loss at iteration 450 is: 0.5658\n",
      "Loss at iteration 475 is: 0.5654\n",
      "Loss at iteration 500 is: 0.5652\n",
      "Loss at iteration 525 is: 0.5650\n",
      "Loss at iteration 550 is: 0.5649\n",
      "Loss at iteration 575 is: 0.5648\n",
      "Loss at iteration 600 is: 0.5648\n",
      "Loss at iteration 625 is: 0.5648\n",
      "Loss at iteration 650 is: 0.5647\n",
      "Loss at iteration 675 is: 0.5647\n",
      "Loss at iteration 700 is: 0.5647\n",
      "Loss at iteration 725 is: 0.5647\n",
      "Loss at iteration 750 is: 0.5647\n",
      "Loss at iteration 775 is: 0.5647\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, y_train, num_iterations=800, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5c2fc",
   "metadata": {},
   "source": [
    "### 2.2 Testing the Translation\n",
    "â\n",
    "#### k-Nearest Neighbors Algorithm\n",
    "â\n",
    "[k-Nearest neighbors algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) \n",
    "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. \n",
    "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors).\n",
    "â\n",
    "#### Searching for the Translation Embedding\n",
    "Since we're approximating the translation function from English to French embeddings by a linear transformation matrix $\\mathbf{R}$, most of the time we won't get the exact embedding of a French word when we transform embedding $\\mathbf{e}$ of some particular English word into the French embedding space. \n",
    "* This is where $k$-NN becomes really useful! By using $1$-NN with $\\mathbf{eR}$ as input, we can search for an embedding $\\mathbf{f}$ (as a row) in the matrix $\\mathbf{Y}$ which is the closest to the transformed vector $\\mathbf{eR}$\n",
    "\n",
    "#### Cosine Similarity\n",
    "Cosine similarity between vectors $u$ and $v$ calculated as the cosine of the angle between them.\n",
    "The formula is \n",
    "\n",
    "$$\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}$$\n",
    "\n",
    "* $\\cos(u,v)$ = $1$ when $u$ and $v$ lie on the same line and have the same direction.\n",
    "* $\\cos(u,v)$ is $-1$ when they have exactly opposite directions.\n",
    "* $\\cos(u,v)$ is $0$ when the vectors are orthogonal (perpendicular) to each other.\n",
    "\n",
    "#### Distance and similarity are pretty much opposite things.\n",
    "* We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric. \n",
    "* When the cosine similarity increases (towards $1$), the \"distance\" between the two vectors decreases (towards $0$). \n",
    "* We can define the cosine distance between $u$ and $v$ as\n",
    "$$d_{\\text{cos}}(u,v)=1-\\cos(u,v)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0a417",
   "metadata": {},
   "source": [
    "### Function `nearest_neighbor`\n",
    "\n",
    "\n",
    "Inputs:\n",
    "* Vector `v`,\n",
    "* A set of possible nearest neighbors `candidates`\n",
    "* `k` nearest neighbors to find.\n",
    "* The distance metric should be based on cosine similarity.\n",
    "* `cosine_similarity` function is already implemented and imported for you. It's arguments are two vectors and it returns the cosine of the angle between them.\n",
    "* Iterate over rows in `candidates`, and save the result of similarities between current row and vector `v` in a python list. Take care that similarities are in the same order as row vectors of `candidates`.\n",
    "* Using[numpy argsort]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort) to sort the indices for the rows of `candidates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe896e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector we are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    \n",
    "    lst_similarity = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cosine_score = cosine_similarity(v, row)\n",
    "        lst_similarity.append(cosine_score)\n",
    "\n",
    "    sorted_ids = np.argsort(lst_similarity)\n",
    "    sorted_ids = sorted_ids[::-1]\n",
    "\n",
    "    k_idx = sorted_ids[:k]\n",
    "\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f763221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1]\n",
      " [1 0 5]\n",
      " [9 9 9]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0777f2",
   "metadata": {},
   "source": [
    "#### Test Translation and Compute its Accuracy\n",
    "\n",
    "### Function `test_vocabulary`\n",
    "Implementing function `test_vocabulary` which takes in English\n",
    "embedding matrix $X$, French embedding matrix $Y$ and the $R$\n",
    "matrix and returns the accuracy of translations from $X$ to $Y$ by $R$.\n",
    "\n",
    "* Iterate over transformed English word embeddings and check if the\n",
    "closest French word vector belongs to French word that is the actual\n",
    "translation.\n",
    "* Obtain an index of the closest French embedding by using\n",
    "`nearest_neighbor` (with argument `k=1`), and compare it to the index\n",
    "of the English embedding that have just transformed.\n",
    "* Keep track of the number of times of the correct translation.\n",
    "* Calculate accuracy as $$\\text{accuracy}=\\frac{\\#(\\text{correct predictions})}{\\#(\\text{total predictions})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a5c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R, nearest_neighbor=nearest_neighbor):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "    pred = np.dot(X, R)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i, :], Y, cosine_similarity=cosine_similarity)\n",
    "        \n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct / pred.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e690014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec5e5f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.566\n"
     ]
    }
   ],
   "source": [
    "acc_test = test_vocabulary(X_test, y_test, R_train)\n",
    "\n",
    "print('Accuracy on test set: {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455527f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686bcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae091c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882fdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2939c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c3e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c99631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
