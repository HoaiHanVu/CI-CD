{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2259e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vuhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/vuhan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd7788",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843d98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_pos_tweets + all_neg_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55520af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "457f1712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in dictionary: 6370\n",
      "Number of French words in dictionary: 5766\n",
      "Dimensions of each words in dictionary: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of English words in dictionary: {len(en_embeddings_subset)}\")\n",
    "print(f\"Number of French words in dictionary: {len(fr_embeddings_subset)}\")\n",
    "\n",
    "print(f\"Dimensions of each words in dictionary: {len(list(en_embeddings_subset.values())[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92964f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7291668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 1500\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('./data/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f4255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweet, stem=True):\n",
    "    \n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    stopwords_eng = stopwords.words('english')\n",
    "    tweet_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_eng and word not in string.punctuation:\n",
    "            tweet_clean.append(word)\n",
    "    if stem==False:\n",
    "        return tweet_clean\n",
    "    else:    \n",
    "        tweet_stem = []\n",
    "        stemmer = PorterStemmer()\n",
    "        for word in tweet_clean:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweet_stem.append(stem_word)\n",
    "        return tweet_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3867",
   "metadata": {},
   "source": [
    "# Seaching with LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54d27155",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99cfe594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(tweet, en_embeddings, process_tweet=process_tweets):\n",
    "    '''\n",
    "    Input: \n",
    "        - tweet: string\n",
    "        - en_embedddings: dictionary of words embeddings\n",
    "        Ouput:\n",
    "        - doc_embedding: sum of all word embeddings in the input tweet\n",
    "    '''\n",
    "    doc_embedding = np.zeros(vec_dims)\n",
    "    doc_processed = process_tweet(tweet)\n",
    "    for word in doc_processed:\n",
    "        doc_embedding += en_embeddings.get(word, 0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "435acab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83251953,  0.13134766,  0.50927734,  0.10461426,  0.10864258,\n",
       "        0.70581055,  0.13476562,  0.08300781,  0.77539062, -0.16485596])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tweet = 'Document embedding is created by summing up the embeddings of all words in the document.'\n",
    "example_embedding = get_doc_embedding(example_tweet, en_embeddings_subset)\n",
    "\n",
    "example_embedding[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c2903fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05480957, -0.0168457 , -0.62866211, -0.34863281, -0.14282227])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e4c8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vecs(all_docs, en_embeddings, get_doc_embedding=get_doc_embedding):\n",
    "    '''\n",
    "    Input:\n",
    "        - all_docs: list of strings - all tweets in dataset.\n",
    "        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n",
    "    Output:\n",
    "        - matrix_doc_vec: matrix of tweet embeddings.\n",
    "        - idxdoc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n",
    "    '''\n",
    "    \n",
    "    idxdoc_dict = {}\n",
    "    list_doc_vec = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc_embedding = get_doc_embedding(doc, en_embeddings, process_tweet=process_tweets)\n",
    "        idxdoc_dict[i] = doc_embedding\n",
    "        list_doc_vec.append(doc_embedding)\n",
    "    \n",
    "    matrix_doc_vec = np.vstack(list_doc_vec)\n",
    "    \n",
    "    return matrix_doc_vec, idxdoc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78e7db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs, idx_tweet_dict = get_doc_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d97c2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary: 10000\n",
      "Shape of vector documents: (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dictionary: {len(idx_tweet_dict)}')\n",
    "print(f'Shape of vector documents: {doc_vecs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490cad1b",
   "metadata": {},
   "source": [
    "#### Choosing the number of planes\n",
    "\n",
    "* Each plane divides the space to $2$ parts.\n",
    "* So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "* We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ vectors.\n",
    "* For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca80437a",
   "metadata": {},
   "source": [
    "<a name=\"3-4\"></a>\n",
    "### Getting the Hash Number for a Vector\n",
    "\n",
    "For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n",
    "\n",
    "#### Hyperplanes in Vector Spaces\n",
    "* In $3$-dimensional vector space, the hyperplane is a regular plane. In $2$ dimensional vector space, the hyperplane is a line.\n",
    "* Generally, the hyperplane is subspace which has dimension $1$ lower than the original vector space has.\n",
    "* A hyperplane is uniquely defined by its normal vector.\n",
    "* Normal vector $n$ of the plane $\\pi$ is the vector to which all vectors in the plane $\\pi$ are orthogonal (perpendicular in $3$ dimensional case).\n",
    "\n",
    "#### Using Hyperplanes to Split the Vector Space\n",
    "We can use a hyperplane to split the vector space into $2$ parts.\n",
    "* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n",
    "* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n",
    "\n",
    "#### Encoding Hash Buckets\n",
    "* For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n",
    "* When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n",
    "* Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n",
    "* If you calculate the dot product with each plane in the same order for every vector, you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a882e8",
   "metadata": {},
   "source": [
    "<a name=\"ex-9\"></a>\n",
    "### Function `hash_value_of_vector`\n",
    "\n",
    "It is list of `N_UNIVERSES` matrices, each describes its own hash table. Each matrix has `N_DIMS` rows and `N_PLANES` columns. Every column of that matrix is a `N_DIMS`-dimensional normal vector for each of `N_PLANES` hyperplanes which are used for creating buckets of the particular hash table.\n",
    "\n",
    "\n",
    "* First multiply your vector `v`, with a corresponding plane. This will give you a vector of dimension $(1,\\text{N_planes})$.\n",
    "* You will then convert every element in that vector to 0 or 1.\n",
    "* You create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n",
    "* You then compute the unique number for the vector by iterating over `N_PLANES`\n",
    "* Then you multiply $2^i$ times the corresponding bit (0 or 1).\n",
    "* You will then store that sum in the variable `hash_value`.\n",
    "\n",
    "\n",
    "$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3d2ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planes = 10\n",
    "n_dims = vec_dims # 300\n",
    "n_universes = 25  # Number of times to repeat the hashing to improve the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebe083f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "list_planes = [np.random.normal(size=(n_dims, n_planes)) for uni in range(n_universes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d422a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list_planes))\n",
    "\n",
    "list_planes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15d4a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for our vector\n",
    "\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(v, planes)\n",
    "    sign_of_dot = np.sign(dot_product)\n",
    "    \n",
    "    h = np.array([[1 if x >= 0 else 0 for x in np.squeeze(sign_of_dot)]])\n",
    "    h = np.squeeze(h)\n",
    "    \n",
    "    hash_value = 0\n",
    "    \n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        hash_value += (2**i * h[i])\n",
    "        hash_value = int(hash_value)\n",
    "        \n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9843c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash value of this vector and the set of planes ast index 0 is 951\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "exp_vec = np.random.rand(1, 300)\n",
    "exp_plane = list_planes[0]\n",
    "\n",
    "exp_hash_val = hash_value_vector(exp_vec, exp_plane)\n",
    "\n",
    "print(f'The hash value of this vector and the set of planes ast index 0 is {exp_hash_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45228db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash value of this vector and the set of planes ast index 0 is 691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_vec = example_embedding\n",
    "exp_plane = list_planes[1]\n",
    "\n",
    "exp_hash_val = hash_value_vector(exp_vec, exp_plane)\n",
    "\n",
    "print(f'The hash value of this vector and the set of planes ast index 0 is {exp_hash_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f749703",
   "metadata": {},
   "source": [
    "<a name=\"3-5\"></a>\n",
    "### Creating a Hash Table\n",
    "\n",
    "<a name=\"ex-10\"></a>\n",
    "### Function `make_hash_table`\n",
    "\n",
    "Given that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n",
    "\n",
    "\n",
    "`make_hash_table` function, which maps the tweet vectors to a bucket and stores the vector there. It returns the `hash_table` and the `id_table`. The `id_table` allows to know which vector in a certain bucket corresponds to what tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1d7964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes, hash_value_vector=hash_value_vector):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - vecs: list of vectors to be hashed.\n",
    "        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "    Output:\n",
    "        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "        - id_table: dictionary - keys are hashes, values are list of vectors id's\n",
    "                            (it's used to know which tweet corresponds to the hashed vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_planes = planes.shape[1]\n",
    "    num_buckets = 2**num_planes\n",
    "    \n",
    "    hash_table = {i: [] for i in range(num_buckets)}\n",
    "    id_table = {i: [] for i in range(num_buckets)}\n",
    "    \n",
    "    for i, v in enumerate(vecs):\n",
    "        h = hash_value_vector(v, planes)\n",
    "        \n",
    "        hash_table[h].append(v) # store the vector into hash_table at key h,\n",
    "                                # by appending the vector v to the list at key h\n",
    "        \n",
    "        id_table[h].append(i) # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n",
    "                              # the key is the h, and the 'i' is appended to the list at key h\n",
    "            \n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5e8cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash table at key 0 has 6 document vectors\n",
      "The id table at key 0 has 6 document indices\n",
      "The first 5 document indices stored at key 0 of id table: [945, 1848, 3421, 6462, 9513]\n"
     ]
    }
   ],
   "source": [
    "exp_hash_table, exp_id_table = make_hash_table(doc_vecs, exp_plane, hash_value_vector=hash_value_vector)\n",
    "\n",
    "print('The hash table at key 0 has {} document vectors'.format(len(exp_hash_table[0])))\n",
    "print('The id table at key 0 has {} document indices'.format(len(exp_id_table[0])))\n",
    "print('The first 5 document indices stored at key 0 of id table: {}'.format(exp_id_table[0][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bdfe4",
   "metadata": {},
   "source": [
    "### Creating all Hash Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9500d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_id_tables(universes):\n",
    "    hash_tables = []\n",
    "    id_tables = []\n",
    "    for id_universe in range(universes):\n",
    "        print('Working on universe: {}'.format(id_universe))\n",
    "        planes = list_planes[id_universe]\n",
    "        hash_table, id_table = make_hash_table(doc_vecs, planes)\n",
    "        \n",
    "        hash_tables.append(hash_table)\n",
    "        id_tables.append(id_table)\n",
    "    return hash_tables, id_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43741360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on universe: 0\n",
      "Working on universe: 1\n",
      "Working on universe: 2\n",
      "Working on universe: 3\n",
      "Working on universe: 4\n",
      "Working on universe: 5\n",
      "Working on universe: 6\n",
      "Working on universe: 7\n",
      "Working on universe: 8\n",
      "Working on universe: 9\n",
      "Working on universe: 10\n",
      "Working on universe: 11\n",
      "Working on universe: 12\n",
      "Working on universe: 13\n",
      "Working on universe: 14\n",
      "Working on universe: 15\n",
      "Working on universe: 16\n",
      "Working on universe: 17\n",
      "Working on universe: 18\n",
      "Working on universe: 19\n",
      "Working on universe: 20\n",
      "Working on universe: 21\n",
      "Working on universe: 22\n",
      "Working on universe: 23\n",
      "Working on universe: 24\n"
     ]
    }
   ],
   "source": [
    "hash_tables, id_tables = create_hash_id_tables(n_universes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95763e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    '''\n",
    "    Input:\n",
    "        A: a numpy array which corresponds to a word vector\n",
    "        B: A numpy array which corresponds to a word vector\n",
    "    Output:\n",
    "        cos: numerical number representing the cosine similarity between A and B.\n",
    "    '''\n",
    "    cos = -10    \n",
    "    dot = np.dot(A, B)\n",
    "    normb = np.linalg.norm(B)\n",
    "    \n",
    "    if len(A.shape) == 1: # If A is just a vector, we get the norm\n",
    "        norma = np.linalg.norm(A)\n",
    "        cos = dot / (norma * normb)\n",
    "    else: # If A is a matrix, then compute the norms of the word vectors of the matrix (norm of each row)\n",
    "        norma = np.linalg.norm(A, axis=1)\n",
    "        epsilon = 1.0e-9 # to avoid division by 0\n",
    "        cos = dot / (norma * normb + epsilon)\n",
    "        \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d6c4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v, row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "\n",
    "    # sort the similarity list and get the indices of the sorted list    \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "    \n",
    "    # Reverse the order of the sorted_ids array\n",
    "    sorted_ids = sorted_ids[::-1]\n",
    "    \n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[:k]\n",
    "\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43d887",
   "metadata": {},
   "source": [
    "## Approximate K-NN\n",
    "\n",
    "<a name=\"ex-11\"></a>\n",
    "## Function `approximate_knn`\n",
    "\n",
    "Implement approximate K nearest neighbors using locality sensitive hashing,\n",
    "to search for documents that are similar to a given document at the\n",
    "index `doc_id`.\n",
    "\n",
    "##### Inputs\n",
    "* `doc_id` is the index into the document list `all_tweets`.\n",
    "* `v` is the document vector for the tweet in `all_tweets` at index `doc_id`.\n",
    "* `list_planes` is the list of planes (the global variable created earlier).\n",
    "* `k` is the number of nearest neighbors to search for.\n",
    "* `num_universes_to_use`: to save time, we can use fewer than the total\n",
    "number of available universes.  By default, it's set to `N_UNIVERSES`,\n",
    "which is $25$ for this notebook.\n",
    "* `hash_tables`: list with hash tables for each universe.\n",
    "* `id_tables`: list with id tables for each universe.\n",
    "\n",
    "The `approximate_knn` function finds a subset of candidate vectors that\n",
    "are in the same \"hash bucket\" as the input vector 'v'.  Then it performs\n",
    "the usual k-nearest neighbors search on this subset (instead of searching\n",
    "through all 10,000 tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c67de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, list_planes, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_vector=hash_value_vector):\n",
    "    \n",
    "    list_vecs_to_consider = list()\n",
    "    list_ids_to_consider = list()\n",
    "    set_ids_to_consider = set()\n",
    "    \n",
    "    for id_universe in range(num_universes_to_use):\n",
    "        \n",
    "        planes = list_planes[id_universe]\n",
    "        hash_value = hash_value_vector(v, planes)\n",
    "        \n",
    "        hash_table = hash_tables[id_universe]\n",
    "        list_document_vectors = hash_table[hash_value]\n",
    "        \n",
    "        id_table = id_tables[id_universe]\n",
    "        new_ids_consider = id_table[hash_value]\n",
    "        \n",
    "        for i, new_id in enumerate(new_ids_consider):\n",
    "            if doc_id == new_id:\n",
    "                continue\n",
    "                \n",
    "            if new_id not in set_ids_to_consider:\n",
    "                document_vector_at_i = list_document_vectors[i]\n",
    "                list_vecs_to_consider.append(document_vector_at_i)\n",
    "                list_ids_to_consider.append(new_id)\n",
    "                set_ids_to_consider.add(new_id)\n",
    "                \n",
    "    print('Fast considering {} vectors'.format(len(list_vecs_to_consider)))\n",
    "\n",
    "    arr_vecs_to_consider = np.array(list_vecs_to_consider)\n",
    "    list_nearest_neighbors_idx = nearest_neighbor(v, arr_vecs_to_consider, k=k)\n",
    "\n",
    "    nearest_neighbor_ids = [list_ids_to_consider[idx] for idx in list_nearest_neighbors_idx]\n",
    "        \n",
    "    return nearest_neighbor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf36cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 1\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = doc_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4663c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 266 vectors\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, list_planes, \n",
    "                                       hash_tables, id_tables, k=3,\n",
    "                                       num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c9d78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Nearest neighbors for document: 1\n",
      "\n",
      "** Document contents: \n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "\n",
      "** Nearest neighbors at document id: 1087\n",
      "** Document content: @bwoyblunder @rajudasonline Sorted :). Thanks. Daaru party in my chaddi, bros.\n",
      "\n",
      "** Nearest neighbors at document id: 1857\n",
      "** Document content: @jamestheeight Hey James, thanks for the tweet. Not currently, no :). Let us know if we can help with anything else. -AL\n",
      "\n",
      "** Nearest neighbors at document id: 4366\n",
      "** Document content: @Sukihaikal hahaha okay thank you :)\n"
     ]
    }
   ],
   "source": [
    "print('** Nearest neighbors for document: {}'.format(doc_id))\n",
    "print('\\n** Document contents: \\n{}'.format(doc_to_search))\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print('\\n** Nearest neighbors at document id: {}'.format(neighbor_id))\n",
    "    print('** Document content: {}'.format(all_tweets[neighbor_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd5588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9115abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af096b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeadce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814dda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
