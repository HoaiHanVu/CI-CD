{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06108a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vuhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import linalg\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da6de9",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d07a01",
   "metadata": {},
   "source": [
    "- Clean and tokenize the corpus.\n",
    "\n",
    "- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n",
    "\n",
    "- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec442ade",
   "metadata": {},
   "source": [
    "## 1.1 Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a724503a",
   "metadata": {},
   "source": [
    "First, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.\n",
    "\n",
    "Next, use NLTK's tokenization engine to split the corpus into individual tokens.\n",
    "\n",
    "Finally, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84e3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    data = nltk.word_tokenize(data)\n",
    "    data = [char.lower() for char in data\n",
    "           if char.isalpha()\n",
    "           or char == '.'\n",
    "           or emoji.get_emoji_regexp().search(char)\n",
    "           ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f91efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'happy',\n",
       " 'because',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'continuous',\n",
       " 'bags',\n",
       " 'of',\n",
       " 'word',\n",
       " 'for',\n",
       " 'words',\n",
       " 'embeddings',\n",
       " '❤️',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = 'I am happy because I am learning continuous bags of word for words embeddings ❤️!!!'\n",
    "\n",
    "words = tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8c202",
   "metadata": {},
   "source": [
    "## 1.2 Slicing window of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bd587",
   "metadata": {},
   "source": [
    "Now that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\n",
    "\n",
    "The first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. Recall that for a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6daea7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[i-C:i] + words[i+1:i+C+1]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c1f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'happy'] am\n",
      "['am', 'because'] happy\n",
      "['happy', 'i'] because\n",
      "['because', 'am'] i\n",
      "['i', 'learning'] am\n",
      "['am', 'continuous'] learning\n",
      "['learning', 'bags'] continuous\n",
      "['continuous', 'of'] bags\n",
      "['bags', 'word'] of\n",
      "['of', 'for'] word\n",
      "['word', 'words'] for\n",
      "['for', 'embeddings'] words\n",
      "['words', '❤️'] embeddings\n",
      "['embeddings', '.'] ❤️\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_windows(tokenize(corpus), 1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488948a",
   "metadata": {},
   "source": [
    "## 1.3 Transforming word into vectors for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e41ae",
   "metadata": {},
   "source": [
    "### 1.3.1 Mapping words to indices and indices to words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16dc7da",
   "metadata": {},
   "source": [
    "The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n",
    "\n",
    "To create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6d95f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        data: the data want to pull from\n",
    "    Output:\n",
    "        word2idx: returns dictionary mapping the word to its index\n",
    "        idx2word: returns dictionary mapping the index to its word\n",
    "    \"\"\"\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for k in words:\n",
    "        word2idx[k] = idx\n",
    "        idx2word[idx] = k\n",
    "        idx += 1\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d6835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'am': 1, 'bags': 2, 'because': 3, 'continuous': 4, 'embeddings': 5, 'for': 6, 'happy': 7, 'i': 8, 'learning': 9, 'of': 10, 'word': 11, 'words': 12, '❤️': 13}\n",
      "\n",
      "{0: '.', 1: 'am', 2: 'bags', 3: 'because', 4: 'continuous', 5: 'embeddings', 6: 'for', 7: 'happy', 8: 'i', 9: 'learning', 10: 'of', 11: 'word', 12: 'words', 13: '❤️'}\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word = get_dict(words)\n",
    "\n",
    "print(word2idx)\n",
    "print()\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df4b9e1",
   "metadata": {},
   "source": [
    "### 1.3.2 Getting one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83648521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, word2idx, V):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the letter of corpus want to transform into one hot vector\n",
    "        word2idx: dictionary with key is word and value is index of word in string\n",
    "        V: size of vocabulary\n",
    "    Output:\n",
    "        one_hot_vector: a vector one hot of word\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2idx[word]] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0518a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_one_hot_vector('word', word2idx, len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b72619",
   "metadata": {},
   "source": [
    "### 1.3.3 Getting context words vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b9727",
   "metadata": {},
   "source": [
    "To create the vectors that represent context words, we will calculate the average of the one-hot vectors representing the individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bcaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_words_to_vector(context_words, word2idx, V):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        context_words: list of context words\n",
    "        word2idx: dictionary with key is word and value is index of word in string\n",
    "        V: size of vocabulary\n",
    "    Output:\n",
    "        context_words_vector: vectors of all context words\n",
    "    \"\"\"\n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2idx, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    \n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe76e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_words_to_vector(['am', 'continuous'], word2idx, len(word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685f326",
   "metadata": {},
   "source": [
    "## 1.4 Building the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1e0f9",
   "metadata": {},
   "source": [
    "To do this we need to use the sliding window function (`get_windows`) to extract the context words and center words, and we then convert these sets of words into a basic vector representation using `word_to_one_hot_vector` and `context_words_to_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf8c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_example(words, C, word2idx, V):\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        yield context_words_to_vector(context_words, word2idx, V),\\\n",
    "              word_to_one_hot_vector(center_word, word2idx, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9881506",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context words vector:  [0.   0.25 0.   0.25 0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.  ]\n",
      "Center word vector:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.   0.5  0.   0.   0.   0.   0.   0.25 0.25 0.   0.   0.   0.   0.  ]\n",
      "Center word vector:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.   0.25 0.   0.25 0.   0.   0.   0.25 0.   0.25 0.   0.   0.   0.  ]\n",
      "Center word vector:  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Context words vector:  [0.   0.   0.   0.25 0.25 0.   0.   0.   0.25 0.25 0.   0.   0.   0.  ]\n",
      "Center word vector:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V = len(word2idx)\n",
    "for context_words_vector, center_word_vector in get_training_example(words[:8], 2, word2idx, V):\n",
    "    print(f'Context words vector:  {context_words_vector}')\n",
    "    print(f'Center word vector:  {center_word_vector}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6498a5",
   "metadata": {},
   "source": [
    "## 1.5 Deviding training set into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32abec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(words, word2idx):\n",
    "    idx = []\n",
    "    for word in words:\n",
    "        idx = idx + [word2idx[word]]\n",
    "    return idx\n",
    "\n",
    "def pack_idx_with_frequency(context_words, word2idx):\n",
    "    freq_dict = defaultdict(int)\n",
    "    for word in context_words:\n",
    "        freq_dict[word] += 1\n",
    "    idxs = get_idx(context_words, word2idx)\n",
    "    packed = []\n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        freq = freq_dict[context_words[i]]\n",
    "        packed.append((idx, freq))\n",
    "    return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ba718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(data, word2idx, V, C):\n",
    "    i = C\n",
    "    while True:\n",
    "        y = np.zeros(V)\n",
    "        x = np.zeros(V)\n",
    "        center_word = data[i]\n",
    "        y[word2idx[center_word]] = 1\n",
    "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
    "        num_ctx_words = len(context_words)\n",
    "        for idx, freq in pack_idx_with_frequency(context_words, word2idx):\n",
    "            x[idx] = freq / num_ctx_words\n",
    "        yield x, y\n",
    "        i += 1\n",
    "        if i >= len(data) - C:\n",
    "#             print(\"i is being set to\", C)\n",
    "            i = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06d3414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, word2idx, V, C, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for x, y in get_vectors(data, word2idx, V, C):\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        else:\n",
    "            yield np.array(batch_x).T, np.array(batch_y).T\n",
    "            batch_x = []\n",
    "            batch_y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a99e1c",
   "metadata": {},
   "source": [
    "# 2. The shallow neurals network for continuous bag-of-words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e934b92",
   "metadata": {},
   "source": [
    "## 2.1 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d3d63",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6272fe0",
   "metadata": {},
   "source": [
    "ReLU is used to calculate the values of the hidden layer, in the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\ \\\\\n",
    " \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1})  \\ \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7d7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b98b8f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25459881],\n",
       "       [ 4.50714306],\n",
       "       [ 2.31993942],\n",
       "       [ 0.98658484],\n",
       "       [-3.4398136 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "z1 = 10 * np.random.rand(5, 1) - 5\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4eff672e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.50714306],\n",
       "       [2.31993942],\n",
       "       [0.98658484],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b6cdb",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95472c61",
   "metadata": {},
   "source": [
    "The second activation function that we need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\\\\\\n",
    " \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\\\\\\n",
    "\\end{align}\n",
    "\n",
    "To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n",
    "\n",
    "$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f41810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4bbe7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = np.array([9, 8, 11, 10, 8.5])\n",
    "\n",
    "softmax(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "603215ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(z1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee56ebc",
   "metadata": {},
   "source": [
    "## 2.2 Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be3b2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    loss = np.sum(np.multiply(np.log(yhat), y))\n",
    "    \n",
    "    cost = -1/batch_size * loss\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39ba42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = softmax(relu(z1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "537ab803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.11385871598675"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(z1, y_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5424beed",
   "metadata": {},
   "source": [
    "## 2.3 Foward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554eae12",
   "metadata": {},
   "source": [
    "### 2.3.1 Initialization of the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0734c",
   "metadata": {},
   "source": [
    "- The first matrix ($W_1$) is of dimension $N \\times V$, where $V$ is the number of words in vocabulary and $N$ is the dimension of word vector.\n",
    "- The second matrix ($W_2$) is of dimension $V \\times N$. \n",
    "- Vector $b_1$ has dimensions $N\\times 1$\n",
    "- Vector $b_2$ has dimensions  $V\\times 1$. \n",
    "- $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7232f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(word2idx)\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cb15452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N, V, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    W1 = np.random.rand(N, V)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a9b42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    h = np.dot(W1, x) + b1\n",
    "    h = relu(h)\n",
    "    z = np.dot(W2, h) + b2\n",
    "    \n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "880d653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_exp = get_training_example(words, 2, word2idx, V)\n",
    "\n",
    "x_arr, y_arr = next(training_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ff9612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X array: [0.   0.25 0.   0.25 0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.  ]\n",
      "Y array [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('X array:', x_arr)\n",
    "print('Y array', y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f168d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.25],\n",
       "       [0.  ],\n",
       "       [0.25],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.5 ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ],\n",
       "       [0.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_arr.copy().reshape((V, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d0a6eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y_arr.copy().reshape((V, 1))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3681d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, b1, b2 = initialize_model(N, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec6034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.50367487]\n",
      " [2.23794117]\n",
      " [2.04583807]\n",
      " [2.34063318]\n",
      " [1.99096705]\n",
      " [1.29042984]\n",
      " [1.90126297]\n",
      " [1.81749928]\n",
      " [1.2244145 ]\n",
      " [2.19518956]\n",
      " [1.62344649]\n",
      " [1.3229408 ]\n",
      " [1.34907424]\n",
      " [0.92805151]]\n"
     ]
    }
   ],
   "source": [
    "z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd21efeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99888303],\n",
       "       [0.64829588],\n",
       "       [0.9361567 ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b51e7879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.606097345651427"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(y, softmax(z), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964c4b",
   "metadata": {},
   "source": [
    "## 2.4 Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9813e6a",
   "metadata": {},
   "source": [
    "The formulas will implement for backpropagation are the following.\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) \\\\\\\n",
    " \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd50e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    \n",
    "    grad_W1 = 1/batch_size * np.dot(relu(np.dot(W2.T, yhat - y)), x.T)\n",
    "    grad_W2 = 1/batch_size * np.dot(yhat - y, h.T)\n",
    "    grad_b1 = 1/batch_size * np.dot(relu(np.dot(W2.T, yhat -y)), np.ones((batch_size, 1)))\n",
    "    grad_b2 = 1/batch_size * np.dot(yhat - y, np.ones((batch_size, 1)))\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58f1743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = softmax(z)\n",
    "\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11cffb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 14\n",
      "Shape of W1: (3, 14)\n",
      "Shape of W2: (14, 3)\n",
      "Shape of b1: (3, 1)\n",
      "Shape of b2: (14, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Size of vocabulary: {}'.format(V))\n",
    "print('Shape of W1: {}'.format(grad_W1.shape))\n",
    "print('Shape of W2: {}'.format(grad_W2.shape))\n",
    "print('Shape of b1: {}'.format(grad_b1.shape))\n",
    "print('Shape of b2: {}'.format(grad_b2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b177c80",
   "metadata": {},
   "source": [
    "## 2.5 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbad56",
   "metadata": {},
   "source": [
    "During the gradient descent phase, we will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\\\\\\n",
    " \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\\\\\\n",
    " \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\\\\\\n",
    " \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40ecc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2idx, N, V, num_iters, alpha, batch_sizes,\n",
    "                     initialize_model=initialize_model, get_batches=get_batches, forward_prop=forward_prop,\n",
    "                     softmax=softmax, compute_cost=compute_cost, back_prop=back_prop, random_seed=42):\n",
    "    '''\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text/list of words\n",
    "        word2idx:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N, V, random_seed=random_seed)\n",
    "    batch_size = batch_sizes\n",
    "    \n",
    "    iters = 0\n",
    "    C = 2\n",
    "    \n",
    "    for x, y in get_batches(data, word2idx, V, C, batch_size):\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        \n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if (iters) % 10 == 0 and iters != 0:\n",
    "            print('Iters: {} cost: {:.5f}'.format(iters, cost))\n",
    "        \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "        \n",
    "        iters += 1\n",
    "        if iters == num_iters:\n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8e97e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters: 10 cost: 2.41922\n",
      "Iters: 20 cost: 2.90396\n",
      "Iters: 30 cost: 3.13578\n",
      "Iters: 40 cost: 3.26347\n",
      "Iters: 50 cost: 2.28515\n",
      "Iters: 60 cost: 2.50531\n",
      "Iters: 70 cost: 2.30940\n",
      "Iters: 80 cost: 2.49927\n",
      "Iters: 90 cost: 2.62349\n",
      "Iters: 100 cost: 2.79418\n",
      "Iters: 110 cost: 2.23276\n",
      "Iters: 120 cost: 2.49900\n",
      "Iters: 130 cost: 2.32018\n",
      "Iters: 140 cost: 2.41170\n",
      "Iters: 150 cost: 2.48986\n",
      "Iters: 160 cost: 2.63515\n",
      "Iters: 170 cost: 2.23168\n",
      "Iters: 180 cost: 2.52124\n",
      "Iters: 190 cost: 2.36628\n",
      "Iters: 200 cost: 2.37304\n",
      "Iters: 210 cost: 2.38228\n",
      "Iters: 220 cost: 2.52324\n",
      "Iters: 230 cost: 2.26360\n",
      "Iters: 240 cost: 2.54000\n",
      "Iters: 250 cost: 2.40891\n",
      "Iters: 260 cost: 2.39060\n",
      "Iters: 270 cost: 2.32906\n",
      "Iters: 280 cost: 2.50040\n",
      "Iters: 290 cost: 2.29496\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "V = len(word2idx)\n",
    "num_iters = 300\n",
    "alpha = 0.03\n",
    "batch_sizes = 1\n",
    "\n",
    "W1, W2, b1, b2 = gradient_descent(words, word2idx, N, V, num_iters, alpha, batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635c12f",
   "metadata": {},
   "source": [
    "# 3. Extracting & Visualizing word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651f489",
   "metadata": {},
   "source": [
    "### Option 1: extract embedding vectors from $\\mathbf{W_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e009cfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79487258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29646731,  0.92134604,  0.67322541,  0.59351037,  0.13294828,\n",
       "         0.14753321, -0.0632072 ,  0.86589602,  0.59106657,  0.6608144 ,\n",
       "        -0.0612544 ,  0.93837818,  0.71961313,  0.12580499],\n",
       "       [ 0.18182497, -0.01217035,  0.23453102,  0.4830255 ,  0.38942226,\n",
       "         0.23884338,  0.59591917,  0.06794761,  0.17929755,  0.27351354,\n",
       "         0.33620639,  0.73243669,  0.13135429,  0.51423444],\n",
       "       [ 0.59241457, -0.02267047,  0.607166  ,  0.04732807,  0.00511449,\n",
       "         0.86076138,  0.91912919,  0.74267101,  0.24119741, -0.02506037,\n",
       "         0.64261171,  0.35202833,  0.08041692,  0.44867407],\n",
       "       [ 0.03438852,  0.79155724,  0.20415943,  0.6067866 ,  0.27870388,\n",
       "         0.51805758,  0.49056391,  0.13093184,  0.87206361,  0.75067889,\n",
       "         0.91528011,  0.86248629,  0.57388791,  0.91986379],\n",
       "       [ 0.07369392,  0.11812929, -0.09589932,  0.2972626 ,  0.26254565,\n",
       "         0.27134903,  0.72259815,  0.32868559,  0.25384416,  0.45322362,\n",
       "         0.04851076,  0.7031557 ,  0.02874332,  0.97208835],\n",
       "       [ 0.75684689,  0.06316786,  0.00552212,  0.73166165,  0.68148896,\n",
       "         0.6772796 ,  0.75587247,  0.00567837,  0.23729247,  0.06069261,\n",
       "         0.81137586,  0.57157056,  0.26377258,  0.04816047],\n",
       "       [ 0.31098232,  0.2592993 ,  0.66707364,  0.60179553,  0.79822436,\n",
       "         0.41530125,  0.08718378,  0.71324479,  0.65913909,  0.53482135,\n",
       "         0.68759766,  0.37801562,  0.46581916,  0.42754102],\n",
       "       [ 0.02541913, -0.04652125, -0.03973275,  0.60907981,  0.19748185,\n",
       "         0.50851424,  0.90756647,  0.19400156,  0.31362443,  0.67778131,\n",
       "         0.17830249,  0.02884775,  0.289695  ,  0.16122129],\n",
       "       [ 0.92969765,  0.80484395,  0.62919429,  0.82983326,  0.69912161,\n",
       "         0.08001342,  0.85890431,  0.53934224,  0.76513659,  0.78917458,\n",
       "         0.17661916, -0.05963621,  0.15228253,  0.39482844],\n",
       "       [ 0.81801477,  0.72542441, -0.10443147,  0.51048552,  0.38683852,\n",
       "         0.15417013,  0.0368791 ,  0.29935847,  0.87459666,  0.29395036,\n",
       "         0.40019035,  0.63482693,  0.27402011,  0.94112485]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99a362",
   "metadata": {},
   "source": [
    "The first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.\n",
    "\n",
    "The first, second, etc. words are ordered as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c071d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:  [0.29646731 0.18182497 0.59241457 0.03438852 0.07369392 0.75684689\n",
      " 0.31098232 0.02541913 0.92969765 0.81801477]\n",
      "am:  [ 0.92134604 -0.01217035 -0.02267047  0.79155724  0.11812929  0.06316786\n",
      "  0.2592993  -0.04652125  0.80484395  0.72542441]\n",
      "bags:  [ 0.67322541  0.23453102  0.607166    0.20415943 -0.09589932  0.00552212\n",
      "  0.66707364 -0.03973275  0.62919429 -0.10443147]\n",
      "because:  [0.59351037 0.4830255  0.04732807 0.6067866  0.2972626  0.73166165\n",
      " 0.60179553 0.60907981 0.82983326 0.51048552]\n",
      "continuous:  [0.13294828 0.38942226 0.00511449 0.27870388 0.26254565 0.68148896\n",
      " 0.79822436 0.19748185 0.69912161 0.38683852]\n",
      "embeddings:  [0.14753321 0.23884338 0.86076138 0.51805758 0.27134903 0.6772796\n",
      " 0.41530125 0.50851424 0.08001342 0.15417013]\n",
      "for:  [-0.0632072   0.59591917  0.91912919  0.49056391  0.72259815  0.75587247\n",
      "  0.08718378  0.90756647  0.85890431  0.0368791 ]\n",
      "happy:  [0.86589602 0.06794761 0.74267101 0.13093184 0.32868559 0.00567837\n",
      " 0.71324479 0.19400156 0.53934224 0.29935847]\n",
      "i:  [0.59106657 0.17929755 0.24119741 0.87206361 0.25384416 0.23729247\n",
      " 0.65913909 0.31362443 0.76513659 0.87459666]\n",
      "learning:  [ 0.6608144   0.27351354 -0.02506037  0.75067889  0.45322362  0.06069261\n",
      "  0.53482135  0.67778131  0.78917458  0.29395036]\n",
      "of:  [-0.0612544   0.33620639  0.64261171  0.91528011  0.04851076  0.81137586\n",
      "  0.68759766  0.17830249  0.17661916  0.40019035]\n",
      "word:  [ 0.93837818  0.73243669  0.35202833  0.86248629  0.7031557   0.57157056\n",
      "  0.37801562  0.02884775 -0.05963621  0.63482693]\n",
      "words:  [0.71961313 0.13135429 0.08041692 0.57388791 0.02874332 0.26377258\n",
      " 0.46581916 0.289695   0.15228253 0.27402011]\n",
      "❤️:  [0.12580499 0.51423444 0.44867407 0.91986379 0.97208835 0.04816047\n",
      " 0.42754102 0.16122129 0.39482844 0.94112485]\n"
     ]
    }
   ],
   "source": [
    "for word in word2idx:\n",
    "    word_embedding_vector = W1[:, word2idx[word]]\n",
    "    print('{}:  {}'.format(word, word_embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ada44",
   "metadata": {},
   "source": [
    "### Option 2: extract embedding vectors from $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d595e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78514847,  0.90441464,  0.44723294,  0.89303439,  0.52618256,\n",
       "         0.0017279 ,  0.70221691,  0.66081036,  0.70489065,  1.03789443,\n",
       "         0.86138779,  0.34561109,  0.50954088,  0.27260226],\n",
       "       [ 0.2224908 ,  0.24806641,  0.71459033, -0.00965601,  0.12573573,\n",
       "         0.8757447 ,  0.14207805,  0.67352305,  0.53109243,  0.53089596,\n",
       "         0.97254448,  0.41943493,  1.04160604,  0.64895465],\n",
       "       [ 0.45049662,  0.05304395,  0.63592102,  0.50729057,  0.94739371,\n",
       "         0.94597347,  0.27291   ,  0.57984851,  0.03723984,  0.59130752,\n",
       "         0.92543099,  0.98861039,  0.26973917,  0.66499809],\n",
       "       [ 0.23155445,  0.49238121,  0.631656  ,  0.28844502,  0.96829109,\n",
       "         0.55941876,  0.98713538,  0.34041681,  0.31454778,  0.54851574,\n",
       "         0.25568095,  0.33503269,  0.54358576,  0.6680886 ],\n",
       "       [ 0.21561687,  0.99528516,  0.20072248,  0.63954194,  0.25993206,\n",
       "         0.38231041,  0.63712538,  0.67660207,  0.24480173,  0.23640647,\n",
       "         0.02758894,  0.17362965,  1.03354417,  0.23253607],\n",
       "       [ 0.01900293,  0.25480448,  0.90090312,  0.13989564,  0.64105421,\n",
       "         0.34539498,  0.10425481,  0.23307817,  0.20093001,  0.72267935,\n",
       "         0.92662712,  0.59174756,  0.75942043,  0.39830458],\n",
       "       [ 0.51323852,  0.7563723 ,  0.46076165,  0.71555489,  0.92881516,\n",
       "         0.61606529,  0.16802052,  0.37498195,  0.93371457,  0.46645958,\n",
       "         0.38347742,  1.00112117,  0.59240954,  0.46590701],\n",
       "       [ 0.42915222,  0.86168929,  0.24874897,  0.35152249,  0.58579223,\n",
       "         0.90227309,  0.69197639,  0.78107262,  0.41534805,  0.05695372,\n",
       "         0.91689163,  0.77340487,  0.73772699,  0.37985051],\n",
       "       [-0.01517273,  0.3463524 ,  0.05399861,  1.03612109,  0.61926796,\n",
       "         0.86277308, -0.09994319,  0.76419123,  0.93288133,  0.81884882,\n",
       "         0.89652668,  0.57804805,  0.24152895,  0.43528864],\n",
       "       [ 0.18884514,  0.7585913 ,  0.66561759,  0.30427531,  0.28315213,\n",
       "         0.79592261,  0.19496933,  1.02398756,  0.58997263,  0.32652135,\n",
       "         0.69821287,  0.0461886 ,  0.27902517,  0.45957415]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f78d2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:  [ 0.78514847  0.2224908   0.45049662  0.23155445  0.21561687  0.01900293\n",
      "  0.51323852  0.42915222 -0.01517273  0.18884514]\n",
      "am:  [0.90441464 0.24806641 0.05304395 0.49238121 0.99528516 0.25480448\n",
      " 0.7563723  0.86168929 0.3463524  0.7585913 ]\n",
      "bags:  [0.44723294 0.71459033 0.63592102 0.631656   0.20072248 0.90090312\n",
      " 0.46076165 0.24874897 0.05399861 0.66561759]\n",
      "because:  [ 0.89303439 -0.00965601  0.50729057  0.28844502  0.63954194  0.13989564\n",
      "  0.71555489  0.35152249  1.03612109  0.30427531]\n",
      "continuous:  [0.52618256 0.12573573 0.94739371 0.96829109 0.25993206 0.64105421\n",
      " 0.92881516 0.58579223 0.61926796 0.28315213]\n",
      "embeddings:  [0.0017279  0.8757447  0.94597347 0.55941876 0.38231041 0.34539498\n",
      " 0.61606529 0.90227309 0.86277308 0.79592261]\n",
      "for:  [ 0.70221691  0.14207805  0.27291     0.98713538  0.63712538  0.10425481\n",
      "  0.16802052  0.69197639 -0.09994319  0.19496933]\n",
      "happy:  [0.66081036 0.67352305 0.57984851 0.34041681 0.67660207 0.23307817\n",
      " 0.37498195 0.78107262 0.76419123 1.02398756]\n",
      "i:  [0.70489065 0.53109243 0.03723984 0.31454778 0.24480173 0.20093001\n",
      " 0.93371457 0.41534805 0.93288133 0.58997263]\n",
      "learning:  [1.03789443 0.53089596 0.59130752 0.54851574 0.23640647 0.72267935\n",
      " 0.46645958 0.05695372 0.81884882 0.32652135]\n",
      "of:  [0.86138779 0.97254448 0.92543099 0.25568095 0.02758894 0.92662712\n",
      " 0.38347742 0.91689163 0.89652668 0.69821287]\n",
      "word:  [0.34561109 0.41943493 0.98861039 0.33503269 0.17362965 0.59174756\n",
      " 1.00112117 0.77340487 0.57804805 0.0461886 ]\n",
      "words:  [0.50954088 1.04160604 0.26973917 0.54358576 1.03354417 0.75942043\n",
      " 0.59240954 0.73772699 0.24152895 0.27902517]\n",
      "❤️:  [0.27260226 0.64895465 0.66499809 0.6680886  0.23253607 0.39830458\n",
      " 0.46590701 0.37985051 0.43528864 0.45957415]\n"
     ]
    }
   ],
   "source": [
    "for word in word2idx:\n",
    "    word_embedding_vector = W2.T[:, word2idx[word]]\n",
    "    print('{}:  {}'.format(word, word_embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f972eb8",
   "metadata": {},
   "source": [
    "### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334a01d",
   "metadata": {},
   "source": [
    "**Calculate the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$, and store the result in `W3`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84602c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54080789, 0.91288034, 0.56022917, 0.74327238, 0.32956542,\n",
       "        0.07463056, 0.31950485, 0.76335319, 0.64797861, 0.84935442,\n",
       "        0.4000667 , 0.64199464, 0.61457701, 0.19920362],\n",
       "       [0.20215788, 0.11794803, 0.47456067, 0.23668475, 0.257579  ,\n",
       "        0.55729404, 0.36899861, 0.37073533, 0.35519499, 0.40220475,\n",
       "        0.65437544, 0.57593581, 0.58648016, 0.58159454],\n",
       "       [0.52145559, 0.01518674, 0.62154351, 0.27730932, 0.4762541 ,\n",
       "        0.90336743, 0.5960196 , 0.66125976, 0.13921862, 0.28312357,\n",
       "        0.78402135, 0.67031936, 0.17507804, 0.55683608],\n",
       "       [0.13297149, 0.64196923, 0.41790772, 0.44761581, 0.62349749,\n",
       "        0.53873817, 0.73884965, 0.23567433, 0.5933057 , 0.64959731,\n",
       "        0.58548053, 0.59875949, 0.55873683, 0.79397619],\n",
       "       [0.14465539, 0.55670723, 0.05241158, 0.46840227, 0.26123886,\n",
       "        0.32682972, 0.67986177, 0.50264383, 0.24932294, 0.34481504,\n",
       "        0.03804985, 0.43839268, 0.53114374, 0.60231221],\n",
       "       [0.38792491, 0.15898617, 0.45321262, 0.43577864, 0.66127159,\n",
       "        0.51133729, 0.43006364, 0.11937827, 0.21911124, 0.39168598,\n",
       "        0.86900149, 0.58165906, 0.51159651, 0.22323253],\n",
       "       [0.41211042, 0.5078358 , 0.56391764, 0.65867521, 0.86351976,\n",
       "        0.51568327, 0.12760215, 0.54411337, 0.79642683, 0.50064047,\n",
       "        0.53553754, 0.68956839, 0.52911435, 0.44672401],\n",
       "       [0.22728567, 0.40758402, 0.10450811, 0.48030115, 0.39163704,\n",
       "        0.70539367, 0.79977143, 0.48753709, 0.36448624, 0.36736752,\n",
       "        0.54759706, 0.40112631, 0.51371099, 0.2705359 ],\n",
       "       [0.45726246, 0.57559818, 0.34159645, 0.93297717, 0.65919478,\n",
       "        0.47139325, 0.37948056, 0.65176674, 0.84900896, 0.8040117 ,\n",
       "        0.53657292, 0.25920592, 0.19690574, 0.41505854],\n",
       "       [0.50342995, 0.74200786, 0.28059306, 0.40738042, 0.33499533,\n",
       "        0.47504637, 0.11592421, 0.66167302, 0.73228465, 0.31023586,\n",
       "        0.54920161, 0.34050777, 0.27652264, 0.7003495 ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W3 = (W1+W2.T)/2\n",
    "\n",
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "636471f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:  [0.54080789 0.20215788 0.52145559 0.13297149 0.14465539 0.38792491\n",
      " 0.41211042 0.22728567 0.45726246 0.50342995]\n",
      "am:  [0.91288034 0.11794803 0.01518674 0.64196923 0.55670723 0.15898617\n",
      " 0.5078358  0.40758402 0.57559818 0.74200786]\n",
      "bags:  [0.56022917 0.47456067 0.62154351 0.41790772 0.05241158 0.45321262\n",
      " 0.56391764 0.10450811 0.34159645 0.28059306]\n",
      "because:  [0.74327238 0.23668475 0.27730932 0.44761581 0.46840227 0.43577864\n",
      " 0.65867521 0.48030115 0.93297717 0.40738042]\n",
      "continuous:  [0.32956542 0.257579   0.4762541  0.62349749 0.26123886 0.66127159\n",
      " 0.86351976 0.39163704 0.65919478 0.33499533]\n",
      "embeddings:  [0.07463056 0.55729404 0.90336743 0.53873817 0.32682972 0.51133729\n",
      " 0.51568327 0.70539367 0.47139325 0.47504637]\n",
      "for:  [0.31950485 0.36899861 0.5960196  0.73884965 0.67986177 0.43006364\n",
      " 0.12760215 0.79977143 0.37948056 0.11592421]\n",
      "happy:  [0.76335319 0.37073533 0.66125976 0.23567433 0.50264383 0.11937827\n",
      " 0.54411337 0.48753709 0.65176674 0.66167302]\n",
      "i:  [0.64797861 0.35519499 0.13921862 0.5933057  0.24932294 0.21911124\n",
      " 0.79642683 0.36448624 0.84900896 0.73228465]\n",
      "learning:  [0.84935442 0.40220475 0.28312357 0.64959731 0.34481504 0.39168598\n",
      " 0.50064047 0.36736752 0.8040117  0.31023586]\n",
      "of:  [0.4000667  0.65437544 0.78402135 0.58548053 0.03804985 0.86900149\n",
      " 0.53553754 0.54759706 0.53657292 0.54920161]\n",
      "word:  [0.64199464 0.57593581 0.67031936 0.59875949 0.43839268 0.58165906\n",
      " 0.68956839 0.40112631 0.25920592 0.34050777]\n",
      "words:  [0.61457701 0.58648016 0.17507804 0.55873683 0.53114374 0.51159651\n",
      " 0.52911435 0.51371099 0.19690574 0.27652264]\n",
      "❤️:  [0.19920362 0.58159454 0.55683608 0.79397619 0.60231221 0.22323253\n",
      " 0.44672401 0.2705359  0.41505854 0.7003495 ]\n"
     ]
    }
   ],
   "source": [
    "for word in word2idx:\n",
    "    word_embedding_vector = W3[:, word2idx[word]]\n",
    "    print('{}:  {}'.format(word, word_embedding_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8eb6a9",
   "metadata": {},
   "source": [
    "### Visualizing the Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60a39782",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = (W1 + W2.T) / 2\n",
    "\n",
    "idx = [word2idx[word] for word in list(word2idx.keys())]\n",
    "\n",
    "X_embs = embs[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1bf5d241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54080789, 0.91288034, 0.56022917, 0.74327238, 0.32956542,\n",
       "        0.07463056, 0.31950485, 0.76335319, 0.64797861, 0.84935442,\n",
       "        0.4000667 , 0.64199464, 0.61457701, 0.19920362],\n",
       "       [0.20215788, 0.11794803, 0.47456067, 0.23668475, 0.257579  ,\n",
       "        0.55729404, 0.36899861, 0.37073533, 0.35519499, 0.40220475,\n",
       "        0.65437544, 0.57593581, 0.58648016, 0.58159454],\n",
       "       [0.52145559, 0.01518674, 0.62154351, 0.27730932, 0.4762541 ,\n",
       "        0.90336743, 0.5960196 , 0.66125976, 0.13921862, 0.28312357,\n",
       "        0.78402135, 0.67031936, 0.17507804, 0.55683608],\n",
       "       [0.13297149, 0.64196923, 0.41790772, 0.44761581, 0.62349749,\n",
       "        0.53873817, 0.73884965, 0.23567433, 0.5933057 , 0.64959731,\n",
       "        0.58548053, 0.59875949, 0.55873683, 0.79397619],\n",
       "       [0.14465539, 0.55670723, 0.05241158, 0.46840227, 0.26123886,\n",
       "        0.32682972, 0.67986177, 0.50264383, 0.24932294, 0.34481504,\n",
       "        0.03804985, 0.43839268, 0.53114374, 0.60231221],\n",
       "       [0.38792491, 0.15898617, 0.45321262, 0.43577864, 0.66127159,\n",
       "        0.51133729, 0.43006364, 0.11937827, 0.21911124, 0.39168598,\n",
       "        0.86900149, 0.58165906, 0.51159651, 0.22323253],\n",
       "       [0.41211042, 0.5078358 , 0.56391764, 0.65867521, 0.86351976,\n",
       "        0.51568327, 0.12760215, 0.54411337, 0.79642683, 0.50064047,\n",
       "        0.53553754, 0.68956839, 0.52911435, 0.44672401],\n",
       "       [0.22728567, 0.40758402, 0.10450811, 0.48030115, 0.39163704,\n",
       "        0.70539367, 0.79977143, 0.48753709, 0.36448624, 0.36736752,\n",
       "        0.54759706, 0.40112631, 0.51371099, 0.2705359 ],\n",
       "       [0.45726246, 0.57559818, 0.34159645, 0.93297717, 0.65919478,\n",
       "        0.47139325, 0.37948056, 0.65176674, 0.84900896, 0.8040117 ,\n",
       "        0.53657292, 0.25920592, 0.19690574, 0.41505854],\n",
       "       [0.50342995, 0.74200786, 0.28059306, 0.40738042, 0.33499533,\n",
       "        0.47504637, 0.11592421, 0.66167302, 0.73228465, 0.31023586,\n",
       "        0.54920161, 0.34050777, 0.27652264, 0.7003495 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afe5c67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "522d2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_embs.T)\n",
    "\n",
    "X_reduced = pca.transform(X_embs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69bc3603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c9e039d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
    "for i, word in idx2word.items():\n",
    "    plt.annotate(word, xy=(X_reduced[i, 0], X_reduced[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99f58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3f1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
